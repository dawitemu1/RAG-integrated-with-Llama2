# ğŸ§  Local RAG System with LLaMA 2, and Multimodal Support

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using **LLaMA 2**. It supports processing **PDF, DOCX, and image files**, converts their contents into vector embeddings, stores them in **FAISS**, and retrieves semantically relevant documents to power intelligent Q&A using an LLM.

---

## âœ¨ Features

- ğŸ“„ Accepts **PDF**, **DOCX**, and **Image** files
- ğŸ§  **LLaMA 2** as the LLM backend (via Ollama or Transformers)
- ğŸ” Uses **Sentence Transformers** (`all-MiniLM-L6-v2`) for embedding
- ğŸ—ƒï¸ Stores embeddings in **FAISS** (or Pinecone as alternative)
- ğŸ”— Built with **LangChain** or **Haystack**
- âš™ï¸ API with **FastAPI** or **Flask**
- ğŸ’» Frontend via **Streamlit**, **React**, or **Flutter**

---

## ğŸ› ï¸ Tech Stack

| Component       | Technology Used                      |
|----------------|--------------------------------------|
| LLM            | LLaMA 2 (via [Ollama](https://ollama.com) or Hugging Face) |
| Embeddings     | `sentence-transformers/all-MiniLM-L6-v2` |
| Vector Store   | [FAISS](https://github.com/facebookresearch/faiss) or [Pinecone](https://www.pinecone.io) |
| RAG Framework  | [LangChain](https://www.langchain.com/) or [Haystack](https://haystack.deepset.ai/) |
| Backend        | [FastAPI](https://fastapi.tiangolo.com/) / [Flask](https://flask.palletsprojects.com/) |
| Frontend       | [Streamlit](https://streamlit.io/) / React / Flutter |

---

## ğŸ“¦ Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/llama2-rag-app.git
cd llama2-rag-app


![image](https://github.com/user-attachments/assets/7c9b61fe-4c46-45d4-bbb7-eab73af468a2)
