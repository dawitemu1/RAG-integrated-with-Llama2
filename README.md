# 🧠 Local RAG System with LLaMA 2, FAISS, and Multimodal Support

This project implements a **Retrieval-Augmented Generation (RAG)** pipeline using **LLaMA 2**. It supports processing **PDF, DOCX, and image files**, converts their contents into vector embeddings, stores them in **FAISS**, and retrieves semantically relevant documents to power intelligent Q&A using an LLM.

---

## ✨ Features

- 📄 Accepts **PDF**, **DOCX**, and **Image** files
- 🧠 **LLaMA 2** as the LLM backend (via Ollama or Transformers)
- 🔍 Uses **Sentence Transformers** (`all-MiniLM-L6-v2`) for embedding
- 🗃️ Stores embeddings in **FAISS** (or Pinecone as alternative)
- 🔗 Built with **LangChain** or **Haystack**
- ⚙️ API with **FastAPI** or **Flask**
- 💻 Frontend via **Streamlit**, **React**, or **Flutter**

---

## 🛠️ Tech Stack

| Component       | Technology Used                      |
|----------------|--------------------------------------|
| LLM            | LLaMA 2 (via [Ollama](https://ollama.com) or Hugging Face) |
| Embeddings     | `sentence-transformers/all-MiniLM-L6-v2` |
| Vector Store   | [FAISS](https://github.com/facebookresearch/faiss) or [Pinecone](https://www.pinecone.io) |
| RAG Framework  | [LangChain](https://www.langchain.com/) or [Haystack](https://haystack.deepset.ai/) |
| Backend        | [FastAPI](https://fastapi.tiangolo.com/) / [Flask](https://flask.palletsprojects.com/) |
| Frontend       | [Streamlit](https://streamlit.io/) / React / Flutter |

---

## 📦 Installation

### 1. Clone the Repository

```bash
git clone https://github.com/your-username/llama2-rag-app.git
cd llama2-rag-app
